{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f9d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# cofing: utf-8\n",
    "\n",
    "# from retry import retry\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import math\n",
    "# from logging import getLogger, StreamHandler, Formatter, FileHandler, DEBUG\n",
    "import yaml\n",
    "import os\n",
    "from retry import retry\n",
    "import sys\n",
    "\n",
    "work_dir = os.getcwd()\n",
    "with open(work_dir + '/setting/kenbiya_scraping_config.yaml', 'r') as yml:\n",
    "    config = yaml.safe_load(yml)\n",
    "area_name = sys.argv[1]\n",
    "if area_name == 'tokyo':\n",
    "    base_url = config['base_url_tokyo']\n",
    "elif area_name == 'osaka':\n",
    "    base_url = config['base_url_osaka']\n",
    "elif area_name == 'fukuoka':\n",
    "    base_url = config['base_url_fukuoka']\n",
    "\n",
    "def write_log(log_file, text):\n",
    "    f = open(log_file, 'a', encoding='UTF-8')\n",
    "    f.write(text)\n",
    "    f.close()\n",
    "    print(text)\n",
    "\n",
    "diff_jst_from_utc = 0\n",
    "start_time = dt.datetime.now() + dt.timedelta(hours=diff_jst_from_utc)\n",
    "now_time = (dt.datetime.now() +\n",
    "            dt.timedelta(hours=diff_jst_from_utc)).strftime('%Y%m%d_%H%M')\n",
    "\n",
    "log_dir = work_dir + f'/log/scraping'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = log_dir + f'/{now_time}_log.txt'\n",
    "f = open(log_file, 'w', encoding='UTF-8')\n",
    "f.close()\n",
    "\n",
    "text = 'processing_start_time:' + str(start_time.replace(microsecond=0)) + '\\n'\n",
    "write_log(log_file, text)\n",
    "excution_date = dt.datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "# file_name = 'suumo_baibai'\n",
    "# excution_date = dt.datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "\n",
    "@retry(tries=3, delay=10, backoff=2)\n",
    "def main():\n",
    "    def get_html(url):\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        return soup\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    # 基本ページurl \n",
    "    page = '1'\n",
    "    url = base_url.format(page = page)\n",
    "    write_log(log_file,'base_url:'+url)\n",
    "    # get html\n",
    "    item = get_html(url)\n",
    "\n",
    "    # extract all items\n",
    "    total_rooms = int(re.sub(r\"\\D\", \"\",item.find(True,\"strong\", class_=\"result_num\").get_text()))\n",
    "    max_page = math.floor(total_rooms/50)+ 1\n",
    "    text = f\"max_page:{max_page} \\n\"\n",
    "    write_log(log_file,text)\n",
    "\n",
    "    url_list = []\n",
    "    all_data = []\n",
    "    error_page = []\n",
    "    # 物件URLの取得\n",
    "    data= {}\n",
    "    # for i in tqdm(range(max_page+1)): \n",
    "    for i in tqdm(range(1)): \n",
    "        url = base_url.format(page = str(i))\n",
    "        # get html\n",
    "        item = get_html(url)\n",
    "        text = item.findAll(\"h3\", {\"class\": \"property_unit-title\"})\n",
    "        write_log(log_file,text)\n",
    "        for j in item.findAll(\"h3\", {\"class\": \"property_unit-title\"}):\n",
    "            room_url = 'https://www.kenbiya.com/'+j.find('a').get('href')\n",
    "            write_log(log_file,'room_url'+room_url)\n",
    "            try:\n",
    "                # room_detail_url = room_url + 'bukkengaiyo/?fmlg=t001'\n",
    "                # get html\n",
    "                room_item = get_html(room_url)\n",
    "                # 物件詳細のデータを収集\n",
    "                data[\"mansion_name\"] = room_item.find(\"h1\", {\"class\": \"inner ttl\"}).getText().strip()\n",
    "                write_log(log_file,data[\"mansion_name\"])\n",
    "            except:\n",
    "                write_log(log_file,'error')\n",
    "            #     data[\"price\"] = room_item.find(\"p\", {\"class\": \"mt7 b\"}).getText().strip()\n",
    "            #     data[\"floor_plan\"] = room_item.findAll(\"td\", {\"class\": \"w290 bdCell\"})[1].getText().strip()\n",
    "            #     data[\"total_rooms\"] = room_item.findAll(\"td\", {\"class\": \"w290 bdCell\"})[3].getText().strip()\n",
    "            #     data[\"exclusive_area\"] = room_item.findAll(\"td\", {\"class\": \"w290 bdCell\"})[4].getText().strip()\n",
    "            #     data[\"other_area\"] = room_item.findAll(\"td\", {\"class\": \"w290 bdCell\"})[5].getText().strip()\n",
    "            #     data[\"stories\"] = room_item.findAll(\"td\", {\"class\": \"w290 bdCell\"})[6].getText().strip()\n",
    "            #     data[\"completion\"] = room_item.findAll(\"td\", {\"class\": \"w290 bdCell\"})[7].getText().strip()\n",
    "            #     data[\"adress\"] = room_item.findAll(\"td\", {\"class\": \"w290 bdCell\"})[8].getText().strip()\n",
    "            #     data[\"access\"] = room_item.findAll(\"td\", {\"class\": \"w290 bdCell\"})[9].getText().strip()\n",
    "\n",
    "            #     # 物件詳細概要ページからのデータを取得\n",
    "            #     room_detail_item = get_html(room_detail_url)\n",
    "            #     data[\"move_in_date\"] = room_detail_item.findAll(\"td\", {\"class\": \"w299 bdCell\"})[12].getText().strip()\n",
    "            #     data[\"direction\"] = room_detail_item.findAll(\"td\", {\"class\": \"w299 bdCell\"})[15].getText().strip()\n",
    "            #     data[\"reform\"] = room_detail_item.findAll(\"td\", {\"class\": \"w299 bdCell\"})[16].getText().strip()\n",
    "            #     data[\"ownership\"] = room_detail_item.findAll(\"td\", {\"class\": \"w299 bdCell\"})[23].getText().strip()\n",
    "            #     data[\"use_district\"] = room_detail_item.findAll(\"td\", {\"class\": \"w299 bdCell\"})[24].getText().strip()\n",
    "            #     data[\"url\"] = room_url\n",
    "            #     data[\"log_date\"] = excution_date\n",
    "            #     all_data.append(data)\n",
    "            # except:\n",
    "            #     error_page.append(room_url)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(work_dir+f'/scraping_raw/kenbiya_baibai_{excution_date}.csv',index = False)\n",
    "        \n",
    "    text = 'df_shape:{}\\n'.format(df.shape)\n",
    "    write_log(log_file,text)\n",
    "\n",
    "    end_time = dt.datetime.now() + dt.timedelta(hours=diff_jst_from_utc)\n",
    "    text = 'predicting done.\\nend_time:{}\\n'.format(end_time)\n",
    "    write_log(log_file,text)\n",
    "\n",
    "    processing_time = end_time - start_time\n",
    "    text = 'processing_time:{}\\n'.format(processing_time)\n",
    "    write_log(log_file,text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
